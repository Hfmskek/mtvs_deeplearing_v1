{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/airplane2230/apparel-image-dataset-2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/249M [00:00<?, ?B/s]\n",
      "  0%|          | 1.00M/249M [00:00<02:37, 1.65MB/s]\n",
      "  1%|          | 2.00M/249M [00:00<01:24, 3.06MB/s]\n",
      "  1%|          | 3.00M/249M [00:00<00:57, 4.45MB/s]\n",
      "  2%|▏         | 4.00M/249M [00:01<00:50, 5.10MB/s]\n",
      "  2%|▏         | 6.00M/249M [00:01<00:34, 7.32MB/s]\n",
      "  3%|▎         | 7.00M/249M [00:01<00:32, 7.78MB/s]\n",
      "  3%|▎         | 8.00M/249M [00:01<00:30, 8.16MB/s]\n",
      "  4%|▎         | 9.00M/249M [00:01<00:32, 7.73MB/s]\n",
      "  4%|▍         | 11.0M/249M [00:01<00:28, 8.77MB/s]\n",
      "  5%|▌         | 13.0M/249M [00:01<00:26, 9.31MB/s]\n",
      "  6%|▌         | 14.0M/249M [00:02<00:27, 9.11MB/s]\n",
      "  6%|▋         | 16.0M/249M [00:02<00:26, 9.13MB/s]\n",
      "  7%|▋         | 17.0M/249M [00:02<00:26, 9.25MB/s]\n",
      "  8%|▊         | 19.0M/249M [00:02<00:25, 9.52MB/s]\n",
      "  8%|▊         | 20.0M/249M [00:02<00:27, 8.65MB/s]\n",
      "  9%|▉         | 22.0M/249M [00:03<00:25, 9.20MB/s]\n",
      " 10%|▉         | 24.0M/249M [00:03<00:24, 9.44MB/s]\n",
      " 10%|█         | 25.0M/249M [00:03<00:24, 9.55MB/s]\n",
      " 11%|█         | 27.0M/249M [00:03<00:21, 10.8MB/s]\n",
      " 12%|█▏        | 29.0M/249M [00:03<00:21, 11.0MB/s]\n",
      " 12%|█▏        | 31.0M/249M [00:03<00:21, 10.7MB/s]\n",
      " 13%|█▎        | 33.0M/249M [00:04<00:21, 10.4MB/s]\n",
      " 14%|█▍        | 35.0M/249M [00:04<00:19, 11.7MB/s]\n",
      " 15%|█▍        | 37.0M/249M [00:04<00:18, 11.8MB/s]\n",
      " 16%|█▌        | 39.0M/249M [00:04<00:20, 10.9MB/s]\n",
      " 16%|█▋        | 41.0M/249M [00:04<00:20, 10.7MB/s]\n",
      " 17%|█▋        | 43.0M/249M [00:05<00:20, 10.7MB/s]\n",
      " 18%|█▊        | 45.0M/249M [00:05<00:18, 11.4MB/s]\n",
      " 19%|█▉        | 47.0M/249M [00:05<00:18, 11.7MB/s]\n",
      " 20%|█▉        | 49.0M/249M [00:05<00:18, 11.6MB/s]\n",
      " 21%|██        | 51.0M/249M [00:05<00:17, 12.0MB/s]\n",
      " 21%|██▏       | 53.0M/249M [00:05<00:16, 12.7MB/s]\n",
      " 22%|██▏       | 55.0M/249M [00:06<00:16, 12.4MB/s]\n",
      " 23%|██▎       | 57.0M/249M [00:06<00:14, 13.9MB/s]\n",
      " 24%|██▎       | 59.0M/249M [00:06<00:13, 15.0MB/s]\n",
      " 25%|██▍       | 61.0M/249M [00:06<00:13, 14.4MB/s]\n",
      " 26%|██▌       | 64.0M/249M [00:06<00:11, 16.3MB/s]\n",
      " 27%|██▋       | 66.0M/249M [00:06<00:11, 16.2MB/s]\n",
      " 27%|██▋       | 68.0M/249M [00:06<00:12, 15.7MB/s]\n",
      " 29%|██▊       | 71.0M/249M [00:06<00:10, 18.3MB/s]\n",
      " 29%|██▉       | 73.0M/249M [00:07<00:09, 18.7MB/s]\n",
      " 31%|███       | 76.0M/249M [00:07<00:09, 19.3MB/s]\n",
      " 32%|███▏      | 79.0M/249M [00:07<00:08, 20.2MB/s]\n",
      " 33%|███▎      | 81.0M/249M [00:07<00:09, 19.5MB/s]\n",
      " 34%|███▍      | 84.0M/249M [00:07<00:07, 21.9MB/s]\n",
      " 35%|███▍      | 87.0M/249M [00:07<00:08, 20.1MB/s]\n",
      " 36%|███▌      | 90.0M/249M [00:07<00:08, 20.0MB/s]\n",
      " 37%|███▋      | 93.0M/249M [00:08<00:07, 22.1MB/s]\n",
      " 39%|███▊      | 96.0M/249M [00:08<00:07, 21.5MB/s]\n",
      " 40%|███▉      | 99.0M/249M [00:08<00:07, 21.0MB/s]\n",
      " 41%|████      | 102M/249M [00:08<00:10, 14.3MB/s] \n",
      " 42%|████▏     | 104M/249M [00:08<00:10, 14.6MB/s]\n",
      " 43%|████▎     | 106M/249M [00:08<00:09, 15.1MB/s]\n",
      " 43%|████▎     | 108M/249M [00:09<00:09, 15.9MB/s]\n",
      " 45%|████▍     | 111M/249M [00:09<00:07, 18.2MB/s]\n",
      " 45%|████▌     | 113M/249M [00:09<00:09, 15.8MB/s]\n",
      " 46%|████▌     | 115M/249M [00:09<00:10, 13.5MB/s]\n",
      " 47%|████▋     | 117M/249M [00:09<00:10, 13.5MB/s]\n",
      " 48%|████▊     | 119M/249M [00:09<00:09, 14.7MB/s]\n",
      " 49%|████▊     | 121M/249M [00:09<00:08, 16.1MB/s]\n",
      " 50%|████▉     | 124M/249M [00:10<00:07, 17.8MB/s]\n",
      " 51%|█████     | 126M/249M [00:10<00:07, 17.2MB/s]\n",
      " 52%|█████▏    | 129M/249M [00:10<00:06, 18.8MB/s]\n",
      " 53%|█████▎    | 132M/249M [00:10<00:06, 19.4MB/s]\n",
      " 54%|█████▍    | 135M/249M [00:10<00:05, 21.1MB/s]\n",
      " 55%|█████▌    | 138M/249M [00:10<00:05, 21.3MB/s]\n",
      " 57%|█████▋    | 141M/249M [00:11<00:08, 13.3MB/s]\n",
      " 57%|█████▋    | 143M/249M [00:11<00:07, 14.2MB/s]\n",
      " 59%|█████▊    | 146M/249M [00:11<00:06, 16.0MB/s]\n",
      " 60%|█████▉    | 149M/249M [00:11<00:05, 17.5MB/s]\n",
      " 62%|██████▏   | 153M/249M [00:11<00:05, 19.1MB/s]\n",
      " 63%|██████▎   | 157M/249M [00:11<00:04, 22.5MB/s]\n",
      " 64%|██████▍   | 160M/249M [00:12<00:04, 23.2MB/s]\n",
      " 66%|██████▌   | 163M/249M [00:12<00:04, 22.3MB/s]\n",
      " 67%|██████▋   | 166M/249M [00:12<00:03, 22.8MB/s]\n",
      " 68%|██████▊   | 169M/249M [00:12<00:03, 21.9MB/s]\n",
      " 70%|██████▉   | 173M/249M [00:12<00:03, 24.4MB/s]\n",
      " 71%|███████   | 176M/249M [00:12<00:04, 19.0MB/s]\n",
      " 72%|███████▏  | 179M/249M [00:13<00:03, 20.6MB/s]\n",
      " 73%|███████▎  | 182M/249M [00:13<00:03, 22.4MB/s]\n",
      " 74%|███████▍  | 185M/249M [00:13<00:02, 22.8MB/s]\n",
      " 76%|███████▌  | 188M/249M [00:13<00:02, 22.8MB/s]\n",
      " 77%|███████▋  | 191M/249M [00:13<00:03, 18.9MB/s]\n",
      " 78%|███████▊  | 193M/249M [00:13<00:03, 16.4MB/s]\n",
      " 79%|███████▉  | 196M/249M [00:13<00:02, 18.7MB/s]\n",
      " 80%|███████▉  | 198M/249M [00:14<00:02, 18.5MB/s]\n",
      " 82%|████████▏ | 203M/249M [00:14<00:01, 24.6MB/s]\n",
      " 83%|████████▎ | 207M/249M [00:14<00:01, 26.7MB/s]\n",
      " 84%|████████▍ | 210M/249M [00:14<00:01, 23.7MB/s]\n",
      " 86%|████████▌ | 213M/249M [00:14<00:01, 23.4MB/s]\n",
      " 87%|████████▋ | 216M/249M [00:14<00:01, 24.6MB/s]\n",
      " 88%|████████▊ | 219M/249M [00:14<00:01, 25.0MB/s]\n",
      " 89%|████████▉ | 222M/249M [00:15<00:01, 23.1MB/s]\n",
      " 90%|█████████ | 225M/249M [00:15<00:01, 23.5MB/s]\n",
      " 92%|█████████▏| 228M/249M [00:15<00:00, 24.1MB/s]\n",
      " 93%|█████████▎| 231M/249M [00:15<00:00, 25.2MB/s]\n",
      " 94%|█████████▍| 234M/249M [00:15<00:00, 24.3MB/s]\n",
      " 95%|█████████▌| 237M/249M [00:15<00:00, 23.1MB/s]\n",
      " 96%|█████████▋| 240M/249M [00:15<00:00, 23.9MB/s]\n",
      " 98%|█████████▊| 243M/249M [00:15<00:00, 24.0MB/s]\n",
      " 99%|█████████▉| 246M/249M [00:16<00:00, 19.2MB/s]\n",
      "100%|█████████▉| 248M/249M [00:16<00:00, 18.5MB/s]\n",
      "100%|██████████| 249M/249M [00:16<00:00, 15.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "License(s): CC0-1.0\n",
      "Downloading apparel-image-dataset-2.zip to c:\\Projects\\Python_basic\\6_딥러닝\\4_전이학습\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !kaggle datasets download -d trolukovich/apparel-images-dataset\n",
    "# !kaggle datasets download -d airplane2230/apparel-image-dataset-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 압축 해제\n",
    "# import shutil\n",
    "# shutil.unpack_archive(\"apparel-image-dataset-2.zip\", \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>black</th>\n",
       "      <th>blue</th>\n",
       "      <th>brown</th>\n",
       "      <th>green</th>\n",
       "      <th>red</th>\n",
       "      <th>white</th>\n",
       "      <th>dress</th>\n",
       "      <th>shirt</th>\n",
       "      <th>pants</th>\n",
       "      <th>shorts</th>\n",
       "      <th>shoes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./clothes_dataset\\blue_shorts\\256d854b55ac32ea...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./clothes_dataset\\red_pants\\584f778aece14f07c2...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./clothes_dataset\\green_pants\\ec543ca241cefb2b...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./clothes_dataset\\brown_shorts\\c8db9e0f7010592...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./clothes_dataset\\white_dress\\551373c80717c5b0...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image  black  blue  brown  \\\n",
       "0  ./clothes_dataset\\blue_shorts\\256d854b55ac32ea...    0.0   1.0    0.0   \n",
       "1  ./clothes_dataset\\red_pants\\584f778aece14f07c2...    0.0   0.0    0.0   \n",
       "2  ./clothes_dataset\\green_pants\\ec543ca241cefb2b...    0.0   0.0    0.0   \n",
       "3  ./clothes_dataset\\brown_shorts\\c8db9e0f7010592...    0.0   0.0    1.0   \n",
       "4  ./clothes_dataset\\white_dress\\551373c80717c5b0...    0.0   0.0    0.0   \n",
       "\n",
       "   green  red  white  dress  shirt  pants  shorts  shoes  \n",
       "0    0.0  0.0    0.0    0.0    0.0    0.0     1.0    0.0  \n",
       "1    0.0  1.0    0.0    0.0    0.0    1.0     0.0    0.0  \n",
       "2    1.0  0.0    0.0    0.0    0.0    1.0     0.0    0.0  \n",
       "3    0.0  0.0    0.0    0.0    0.0    0.0     1.0    0.0  \n",
       "4    0.0  0.0    1.0    1.0    0.0    0.0     0.0    0.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"train.csv\", index_col=0)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# 멀티레이블을 위한 커스텀 데이터셋 클래스\n",
    "class ClothesDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.transform = transform\n",
    "        self.image_paths = np.array(df['image'])\n",
    "        self.labels = np.array(df.iloc[:, 1:])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path)\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return torch.tensor(image, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 변환 정의\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>black</th>\n",
       "      <th>blue</th>\n",
       "      <th>brown</th>\n",
       "      <th>green</th>\n",
       "      <th>red</th>\n",
       "      <th>white</th>\n",
       "      <th>dress</th>\n",
       "      <th>shirt</th>\n",
       "      <th>pants</th>\n",
       "      <th>shorts</th>\n",
       "      <th>shoes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./clothes_dataset\\blue_shorts\\1ec871cdfe41ff20...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./clothes_dataset\\black_pants\\0f8eb4509cea01f9...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./clothes_dataset\\white_shoes\\1d70ebaec12e09f3...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./clothes_dataset\\green_shoes\\a4e155f7249a591e...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./clothes_dataset\\blue_shirt\\2edd17185a4f1b8d9...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image  black  blue  brown  \\\n",
       "0  ./clothes_dataset\\blue_shorts\\1ec871cdfe41ff20...    0.0   1.0    0.0   \n",
       "1  ./clothes_dataset\\black_pants\\0f8eb4509cea01f9...    1.0   0.0    0.0   \n",
       "2  ./clothes_dataset\\white_shoes\\1d70ebaec12e09f3...    0.0   0.0    0.0   \n",
       "3  ./clothes_dataset\\green_shoes\\a4e155f7249a591e...    0.0   0.0    0.0   \n",
       "4  ./clothes_dataset\\blue_shirt\\2edd17185a4f1b8d9...    0.0   1.0    0.0   \n",
       "\n",
       "   green  red  white  dress  shirt  pants  shorts  shoes  \n",
       "0    0.0  0.0    0.0    0.0    0.0    0.0     1.0    0.0  \n",
       "1    0.0  0.0    0.0    0.0    0.0    1.0     0.0    0.0  \n",
       "2    0.0  0.0    1.0    0.0    0.0    0.0     0.0    1.0  \n",
       "3    1.0  0.0    0.0    0.0    0.0    0.0     0.0    1.0  \n",
       "4    0.0  0.0    0.0    0.0    1.0    0.0     0.0    0.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"test.csv\", index_col=0)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# 데이터 셋 및 데이터 로더 생성\n",
    "dataset = ClothesDataset(df=train_df, transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = ClothesDataset(df=test_df, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df.columns) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전이학습\n",
    "import torch.nn as nn\n",
    "\n",
    "for name, module in model.named_parameters():\n",
    "    module.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_parameters():\n",
    "    print(module.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(512, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=11, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_17084\\9732327.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(image, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1, loss : 0.2638653062922614\n",
      "epoch : 2, loss : 0.14079637974500656\n",
      "epoch : 3, loss : 0.11270187948431287\n",
      "epoch : 4, loss : 0.10194286833916391\n",
      "epoch : 5, loss : 0.09144565514155796\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "for epochs in range(5):\n",
    "    loss_sum = 0\n",
    "    cnt = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        cnt += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        outputs = torch.sigmoid(outputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_sum += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"epoch : {epochs + 1}, loss : {loss_sum/cnt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_17084\\9732327.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(image, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.205545229244114\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "cnt = 0\n",
    "with torch.no_grad():\n",
    "    for img,label in test_loader:\n",
    "        outputs = model(img.to('cuda'))\n",
    "        greater_than_half = nn.Sigmoid()(model(img.to('cuda'))) > 0.5\n",
    "        result = torch.nonzero(greater_than_half)\n",
    "\n",
    "        label_result = label > 0.5\n",
    "        result1 = torch.nonzero(label_result)\n",
    "\n",
    "        for temp,temp1 in zip(result,result1):\n",
    "            result_data = torch.equal(temp.to('cpu'),temp1.to('cpu'))\n",
    "            if result_data:\n",
    "                correct += 1\n",
    "            cnt += 1\n",
    "\n",
    "\n",
    "    accuracy = correct / cnt\n",
    "    print(f'accuracy : {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1],\n",
       "        [ 0,  8],\n",
       "        [ 1, 10],\n",
       "        [ 2,  1],\n",
       "        [ 2,  6],\n",
       "        [ 3,  1],\n",
       "        [ 3, 10],\n",
       "        [ 4,  3],\n",
       "        [ 4, 10],\n",
       "        [ 5,  1],\n",
       "        [ 5,  8],\n",
       "        [ 6,  3],\n",
       "        [ 6,  8],\n",
       "        [ 7,  8],\n",
       "        [ 8,  1],\n",
       "        [ 8,  8],\n",
       "        [ 9,  2],\n",
       "        [ 9, 10],\n",
       "        [10,  5],\n",
       "        [10,  8],\n",
       "        [11,  4],\n",
       "        [11,  8],\n",
       "        [12,  3],\n",
       "        [12,  7],\n",
       "        [13,  0],\n",
       "        [13, 10],\n",
       "        [14,  0],\n",
       "        [15,  2],\n",
       "        [15, 10],\n",
       "        [16,  5],\n",
       "        [16,  6],\n",
       "        [17,  0],\n",
       "        [18,  1],\n",
       "        [18,  7],\n",
       "        [19,  4],\n",
       "        [19,  8],\n",
       "        [20,  1],\n",
       "        [20,  8],\n",
       "        [21,  8],\n",
       "        [22,  1],\n",
       "        [22,  8],\n",
       "        [23,  4],\n",
       "        [23,  6]], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greater_than_half = nn.Sigmoid()(model(img.to('cuda'))) > 0.5\n",
    "result = torch.nonzero(greater_than_half)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1],\n",
       "        [ 0,  8],\n",
       "        [ 1, 10],\n",
       "        [ 2,  1],\n",
       "        [ 2,  6],\n",
       "        [ 3,  1],\n",
       "        [ 3, 10],\n",
       "        [ 4,  3],\n",
       "        [ 4, 10],\n",
       "        [ 5,  1],\n",
       "        [ 5,  8],\n",
       "        [ 6,  3],\n",
       "        [ 6,  8],\n",
       "        [ 7,  8],\n",
       "        [ 8,  1],\n",
       "        [ 8,  8],\n",
       "        [ 9,  2],\n",
       "        [ 9, 10],\n",
       "        [10,  5],\n",
       "        [10,  8],\n",
       "        [11,  4],\n",
       "        [11,  8],\n",
       "        [12,  3],\n",
       "        [12,  7],\n",
       "        [13,  0],\n",
       "        [13, 10],\n",
       "        [14,  0],\n",
       "        [15,  2],\n",
       "        [15, 10],\n",
       "        [16,  5],\n",
       "        [16,  6],\n",
       "        [17,  0],\n",
       "        [18,  1],\n",
       "        [18,  7],\n",
       "        [19,  4],\n",
       "        [19,  8],\n",
       "        [20,  1],\n",
       "        [20,  8],\n",
       "        [21,  8],\n",
       "        [22,  1],\n",
       "        [22,  8],\n",
       "        [23,  4],\n",
       "        [23,  6]], device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_result = label > 0.5\n",
    "result1 = torch.nonzero(greater_than_half)\n",
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1]) tensor([0, 1])\n",
      "tensor([0, 8]) tensor([0, 8])\n",
      "tensor([ 1, 10]) tensor([ 1, 10])\n",
      "tensor([2, 1]) tensor([2, 1])\n",
      "tensor([2, 6]) tensor([2, 6])\n",
      "tensor([3, 1]) tensor([3, 1])\n",
      "tensor([ 3, 10]) tensor([ 3, 10])\n",
      "tensor([4, 3]) tensor([4, 3])\n",
      "tensor([ 4, 10]) tensor([ 4, 10])\n",
      "tensor([5, 1]) tensor([5, 1])\n",
      "tensor([5, 8]) tensor([5, 8])\n",
      "tensor([6, 3]) tensor([6, 3])\n",
      "tensor([6, 8]) tensor([6, 8])\n",
      "tensor([7, 8]) tensor([7, 8])\n",
      "tensor([8, 1]) tensor([8, 1])\n",
      "tensor([8, 8]) tensor([8, 8])\n",
      "tensor([9, 2]) tensor([9, 2])\n",
      "tensor([ 9, 10]) tensor([ 9, 10])\n",
      "tensor([10,  5]) tensor([10,  5])\n",
      "tensor([10,  8]) tensor([10,  8])\n",
      "tensor([11,  4]) tensor([11,  4])\n",
      "tensor([11,  8]) tensor([11,  8])\n",
      "tensor([12,  3]) tensor([12,  3])\n",
      "tensor([12,  7]) tensor([12,  7])\n",
      "tensor([13,  0]) tensor([13,  0])\n",
      "tensor([13, 10]) tensor([13, 10])\n",
      "tensor([14,  0]) tensor([14,  0])\n",
      "tensor([15,  2]) tensor([15,  2])\n",
      "tensor([15, 10]) tensor([15, 10])\n",
      "tensor([16,  5]) tensor([16,  5])\n",
      "tensor([16,  6]) tensor([16,  6])\n",
      "tensor([17,  0]) tensor([17,  0])\n",
      "tensor([18,  1]) tensor([18,  1])\n",
      "tensor([18,  7]) tensor([18,  7])\n",
      "tensor([19,  4]) tensor([19,  4])\n",
      "tensor([19,  8]) tensor([19,  8])\n",
      "tensor([20,  1]) tensor([20,  1])\n",
      "tensor([20,  8]) tensor([20,  8])\n",
      "tensor([21,  8]) tensor([21,  8])\n",
      "tensor([22,  1]) tensor([22,  1])\n",
      "tensor([22,  8]) tensor([22,  8])\n",
      "tensor([23,  4]) tensor([23,  4])\n",
      "tensor([23,  6]) tensor([23,  6])\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "true_sum = 0\n",
    "\n",
    "for temp,temp1 in zip(result,result1):\n",
    "    print(temp.to('cpu'),temp1.to('cpu'))\n",
    "    result_data = torch.equal(temp.to('cpu'),temp1.to('cpu'))\n",
    "    if result_data:\n",
    "        true_sum += 1\n",
    "print(true_sum/len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1],\n",
       "        [ 0,  8],\n",
       "        [ 1, 10],\n",
       "        [ 2,  1],\n",
       "        [ 2,  6],\n",
       "        [ 3,  1],\n",
       "        [ 3, 10],\n",
       "        [ 4,  3],\n",
       "        [ 4, 10],\n",
       "        [ 5,  1],\n",
       "        [ 5,  8],\n",
       "        [ 6,  3],\n",
       "        [ 6,  8],\n",
       "        [ 7,  8],\n",
       "        [ 8,  1],\n",
       "        [ 8,  8],\n",
       "        [ 9,  2],\n",
       "        [ 9, 10],\n",
       "        [10,  5],\n",
       "        [10,  8],\n",
       "        [11,  4],\n",
       "        [11,  8],\n",
       "        [12,  3],\n",
       "        [12,  7],\n",
       "        [13,  0],\n",
       "        [13, 10],\n",
       "        [14,  0],\n",
       "        [15,  2],\n",
       "        [15, 10],\n",
       "        [16,  5],\n",
       "        [16,  6],\n",
       "        [17,  0],\n",
       "        [18,  1],\n",
       "        [18,  7],\n",
       "        [19,  4],\n",
       "        [19,  8],\n",
       "        [20,  1],\n",
       "        [20,  8],\n",
       "        [21,  8],\n",
       "        [22,  1],\n",
       "        [22,  8],\n",
       "        [23,  4],\n",
       "        [23,  6]], device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "nn.Sigmoid()(model(img.to('cuda')))\n",
    "greater_than_half = nn.Sigmoid()(model(img.to('cuda'))) > 0.5\n",
    "result = torch.nonzero(greater_than_half)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['image', 'green'], dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns[[0,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1],\n",
       "        [ 0,  8],\n",
       "        [ 1,  0],\n",
       "        [ 1, 10],\n",
       "        [ 2,  1],\n",
       "        [ 2,  6],\n",
       "        [ 3,  1],\n",
       "        [ 3, 10],\n",
       "        [ 4,  3],\n",
       "        [ 4, 10],\n",
       "        [ 5,  1],\n",
       "        [ 5,  8],\n",
       "        [ 6,  3],\n",
       "        [ 6,  8],\n",
       "        [ 7,  0],\n",
       "        [ 7,  8],\n",
       "        [ 8,  1],\n",
       "        [ 8,  8],\n",
       "        [ 9,  2],\n",
       "        [ 9, 10],\n",
       "        [10,  5],\n",
       "        [10,  8],\n",
       "        [11,  4],\n",
       "        [11,  8],\n",
       "        [12,  3],\n",
       "        [12,  7],\n",
       "        [13,  0],\n",
       "        [13, 10],\n",
       "        [14,  0],\n",
       "        [14,  7],\n",
       "        [15,  2],\n",
       "        [15, 10],\n",
       "        [16,  5],\n",
       "        [16,  6],\n",
       "        [17,  0],\n",
       "        [17,  6],\n",
       "        [18,  1],\n",
       "        [18,  7],\n",
       "        [19,  4],\n",
       "        [19,  8],\n",
       "        [20,  1],\n",
       "        [20,  8],\n",
       "        [21,  0],\n",
       "        [21,  8],\n",
       "        [22,  1],\n",
       "        [22,  8],\n",
       "        [23,  4],\n",
       "        [23,  6]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greater_than_half = label > 0.5\n",
    "result = torch.nonzero(greater_than_half)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>black</th>\n",
       "      <th>blue</th>\n",
       "      <th>brown</th>\n",
       "      <th>green</th>\n",
       "      <th>red</th>\n",
       "      <th>white</th>\n",
       "      <th>dress</th>\n",
       "      <th>pants</th>\n",
       "      <th>shirt</th>\n",
       "      <th>shoes</th>\n",
       "      <th>short</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>path_to_image1.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>path_to_image2.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>path_to_image3.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>path_to_image4.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                image  black  blue  brown  green  red  white  dress  pants  \\\n",
       "0  path_to_image1.jpg      1     0      0      0    1      0      1      0   \n",
       "1  path_to_image2.jpg      0     1      1      0    0      1      0      1   \n",
       "2  path_to_image3.jpg      0     1      0      1    0      0      0      1   \n",
       "3  path_to_image4.jpg      1     0      0      0    1      0      1      0   \n",
       "\n",
       "   shirt  shoes  short  \n",
       "0      0      0      0  \n",
       "1      0      0      0  \n",
       "2      1      0      0  \n",
       "3      0      1      0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예시 데이터프레임 생성\n",
    "data = {\n",
    "    'image': ['path_to_image1.jpg', 'path_to_image2.jpg', 'path_to_image3.jpg', 'path_to_image4.jpg'],\n",
    "    'black': [1, 0, 0, 1],\n",
    "    'blue': [0, 1, 1, 0],\n",
    "    'brown': [0, 1, 0, 0],\n",
    "    'green': [0, 0, 1, 0],\n",
    "    'red': [1, 0, 0, 1],\n",
    "    'white': [0, 1, 0, 0],\n",
    "    'dress': [1, 0, 0, 1],\n",
    "    'pants': [0, 1, 1, 0],\n",
    "    'shirt': [0, 0, 1, 0],\n",
    "    'shoes': [0, 0, 0, 1],\n",
    "    'short': [0, 0, 0, 0]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gumi_env310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
